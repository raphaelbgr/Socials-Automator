# Socials Automator - Provider Configuration
# ============================================
# This file configures AI providers for text and image generation.
# LiteLLM handles all text providers with a unified API.
# Environment variables are loaded from .env file.

provider_settings:
  timeout_seconds: 60
  max_retries: 3
  retry_delay_seconds: 2
  fallback_on_error: true

# ============================================
# LLM FALLBACK CONFIGURATION
# ============================================
# Controls automatic retry and provider switching behavior.
# Local LLMs (LM Studio, Ollama) get more retries since they may
# need multiple attempts for valid JSON output.

llm_fallback:
  # Retry counts by provider type
  local_max_retries: 10      # LM Studio, Ollama - more retries for JSON issues
  external_max_retries: 5    # Cloud APIs - fewer retries, more reliable

  # Which providers are considered "local" (get more retries)
  local_providers:
    - lmstudio
    - ollama

  # Fallback priority order (when preferred provider exhausted)
  # Comment out providers you don't have API keys for
  fallback_priority:
    - zai           # Z.AI GLM-4.5-Air (cheap, fast)
    - gemini        # Google Gemini 2.0 Flash
    - groq          # Groq Llama 3.3 70B
    - openai        # OpenAI GPT-4o (premium fallback)

# ============================================
# TEXT PROVIDERS (via LiteLLM)
# ============================================
# LiteLLM uses model prefixes to route to correct provider:
# - openai/gpt-4o
# - anthropic/claude-sonnet-4-20250514
# - groq/llama-3.3-70b-versatile
# - ollama/llama3.2
# - openai/local (for LM Studio with custom base_url)

text_providers:
  # Priority 1: Z.AI GLM-4.5-Air (Cheap & Fast)
  # Model: GLM-4.5-Air - 106B total params, 12B active (MoE)
  # Docs: https://docs.z.ai/
  zai:
    priority: 1
    enabled: true
    litellm_model: "openai/GLM-4.5-Air"
    base_url_env: "ZAI_API_URL"
    api_key_env: "ZAI_API_KEY"
    timeout: 60

  # Priority 2: Groq (Fast inference)
  groq:
    priority: 2
    enabled: true
    litellm_model: "groq/llama-3.3-70b-versatile"
    api_key_env: "GROQ_API_KEY"
    timeout: 30

  # Priority 3: Google Gemini
  gemini:
    priority: 3
    enabled: true
    litellm_model: "gemini/gemini-2.0-flash"
    api_key_env: "GOOGLE_API_KEY"
    timeout: 30

  # Priority 4: OpenAI (Premium)
  openai:
    priority: 4
    enabled: true
    litellm_model: "openai/gpt-4o"
    api_key_env: "OPENAI_API_KEY"
    timeout: 60
    models:
      fast: "openai/gpt-4o-mini"
      quality: "openai/gpt-4o"

  # Priority 5: Anthropic (NOT OpenAI compatible - disabled)
  anthropic:
    priority: 5
    enabled: false  # Needs Anthropic SDK, not OpenAI compatible
    litellm_model: "anthropic/claude-sonnet-4-20250514"
    api_key_env: "ANTHROPIC_API_KEY"
    timeout: 60

  # Local providers
  # NOTE: GLM-4.6v-flash has built-in reasoning (~60-120s thinking time)
  # that cannot be disabled via API. Use a non-reasoning model like
  # hermes-3-llama-3.1-8b or mistral for faster responses.
  lmstudio:
    priority: 10  # Lower priority - ZAI is faster (no thinking overhead)
    enabled: true  # Auto-detects loaded model
    litellm_model: "openai/local-model"
    base_url: "http://localhost:1234/v1"
    api_key: "lm-studio"
    timeout: 180  # 3 min for reasoning models

  ollama:
    priority: 11
    enabled: false
    litellm_model: "ollama/llama3.2"
    base_url: "http://localhost:11434"
    timeout: 120

# Default text provider chain (fallback order)
text_priority_chain:
  - zai
  - groq
  - gemini
  - openai

# ============================================
# IMAGE PROVIDERS
# ============================================
image_providers:
  # DALL-E 3 - Currently working (OpenAI)
  dalle:
    priority: 1
    enabled: true
    type: "openai"
    model: "dall-e-3"
    api_key_env: "OPENAI_API_KEY"
    timeout: 60
    cost_per_image: 0.04
    settings:
      quality: "standard"

  # Nano Banana Pro 2 (requires fal.ai balance)
  nanobanana:
    priority: 2
    enabled: false  # Disabled - fal.ai balance empty
    type: "fal"
    model: "fal-ai/nano-banana-pro"
    api_key_env: "FAL_API_KEY"
    timeout: 90
    cost_per_image: 0.15

  fal_flux:
    priority: 3
    enabled: false  # Disabled - fal.ai balance empty
    type: "fal"
    model: "fal-ai/flux/schnell"
    api_key_env: "FAL_API_KEY"
    timeout: 60
    cost_per_image: 0.01

  replicate:
    priority: 4
    enabled: false  # Disabled - had errors
    type: "replicate"
    model: "stability-ai/sdxl:latest"
    api_key_env: "REPLICATE_API_TOKEN"
    timeout: 120
    cost_per_image: 0.03

  # ComfyUI - Local Stable Diffusion (workflow-based)
  # High quality settings for local generation (no API costs)
  comfyui:
    priority: 5
    enabled: true
    type: "comfyui"
    model: "local"  # Uses whatever model is loaded in ComfyUI
    base_url_env: "COMFYUI_BASE_URL"
    timeout: 300  # 5 minutes for high quality generation
    cost_per_image: 0.0
    settings:
      # High quality generation settings
      steps: 35  # More steps = better quality (default was 20)
      cfg: 7.5  # Classifier-free guidance scale
      sampler: "dpmpp_2m_sde"  # High quality sampler
      scheduler: "karras"  # Better noise scheduling
      # Workflow settings (optional)
      workflow_file: null
      prompt_node_id: null
      negative_prompt_node_id: null

# Default image provider chain (fallback order)
image_priority_chain:
  - dalle
  - comfyui  # Local option if ComfyUI running

# ============================================
# TASK-SPECIFIC OVERRIDES
# ============================================
# Override provider for specific tasks
task_overrides:
  # All tasks use Z.AI GLM-4.5-Air (cheapest)
  research:
    text_provider: zai

  hook_generation:
    text_provider: zai
    temperature: 0.9

  content_planning:
    text_provider: openai  # GPT-4o is better at following structured instructions
    temperature: 0.7

  # Images use fal.ai (you have FAL_API_KEY)
  hook_images:
    image_provider: fal_flux

  content_images:
    image_provider: fal_flux
